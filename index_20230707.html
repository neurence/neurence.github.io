<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Catchup 20230707</title>
    <style>
        body {
            font-family: sans-serif;
        }

        div {
            margin: 20px;
        }
        table {
            background: #c7e3ff;
            padding: 10px;
        }
    </style>
</head>
<body>
<div>
    <h1>Current Audio Work</h1>
    <div style="background-color: #9cf1ce; padding: 10px">

        <h2>Use an earlier hidden state of the transformer, more field recordings</h2>
        <p>The Hubert transformer has 24 transformer blocks.  We've run an experiment to use the 20th block rather than the final (24th) block as input to the decoder. <br/>Sample outputs are below:</p>
        <h3>Pub recordings (Take 2 - 5th July)</h3>

        <div>
            <p>Original Recording</p>
            <audio controls src="media/cambridge_pub_recordings/pub_recordings/sample1_050723_185337/noisy.wav"></audio>
            <audio controls src="media/cambridge_pub_recordings/pub_recordings/sample2_050723_183053/noisy.wav"></audio>

            <br/>

            <p>Denoised</p>
            <audio controls src="media/cambridge_pub_recordings/pub_recordings/sample1_050723_185337/denoised.wav"></audio>
            <audio controls src="media/cambridge_pub_recordings/pub_recordings/sample2_050723_183053/denoised.wav"></audio>
            <br/>

            <p>Denoised, then autoencoded with network trained on multi-speaker dataset (LibriSpeech)</p>
            <audio controls src="media/cambridge_pub_recordings/pub_recordings/sample1_050723_185337/hubert_large_denoised.wav"></audio>
            <audio controls src="media/cambridge_pub_recordings/pub_recordings/sample2_050723_183053/hubert_large_denoised.wav"></audio>
            <br/>

        </div>
    </div>
        <div style="background-color: #f1eda8; padding: 10px">
        <h2>Add Speaker embedding (no outputs available yet - experiment is at early stage) </h2>
        <p>Based on the fact that people use knowledge of the speaker's voice to aid in listening,  we've added a learned speaker embedding to the encoder output, which may improve the decoding stage. </p>
    </div>
    <div style="background-color: #c7eef2; padding: 10px">
        <h2>Proof of concept for using speaker embeddings for voice-changing the autoencoder output.</h2>
        <p>Using the <a href="https://arxiv.org/abs/2210.15418">FreeVC</a> one-shot voice conversion framework, we adapted the outputs of the autoencoder using embeddings derived at runtime from the denoised input.</p>
        <p>Denoised Audio</p>
        <audio controls src="media/input_samples/valid_female_denoised_020.wav"></audio>
        <br/>
        <p>Denoised, then autoencoded with network trained on single-speaker dataset (LJSpeech)</p>
        <audio controls src="media/hubert/hubert_hsXX/370/out_denoised_female_020.wav"></audio>
        <br/>
        <p>Denoised -> autoencoded -> voice-changed</p>
        <audio controls src="media/freevc/outn3xa47rv.wav"></audio>
        <br/>

    </div>
    <br/>
    <hr/>
    <div>
        Previous catch-ups:
        <a href="index_20230630.html">2023-06-30</a>
        <a href="index_20230623.html">2023-06-23</a>
        <a href="index_20230616.html">2023-06-16</a>
        <a href="index_20230609.html">2023-06-09</a>
        <a href="index_20230602.html">2023-06-02</a>
        <a href="index_20230526.html">2023-05-26</a>
    </div>

</body>
</html>
