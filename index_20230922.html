<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Catchup 20230922</title>
    <style>
        body {
            font-family: sans-serif;
        }

        div {
            margin: 20px;
        }

        div.section {
            background: #c7e3ff;
            padding: 10px;
        }

        div.center {
            text-align: center;
        }
        #older {
            display: none;
        }
        #older:target {
            display: block;
        }

        table {
            font-size: small;
            background: #88c0e3;
            padding: 10px;
        }

        .num {
            text-align: right;
            border: 1px solid gray;
        }

        th.highlight {}

    </style>
</head>
<body>
<div>
    <div class="section">
        <h2>Porting The AutoEncoder to a Mobile Device</h2>
        <p>Working on putting test app onto testflight for testing out live processing.</p>
        <p>Here is a recording using denoiser and autoencoder with one second input length:</p>
        <p>Input</p>
        <audio controls src="media/test_ios_app_io/recordingInput_220923_123901.wav"></audio>
        <p>Output</p>
        <audio controls src="media/test_ios_app_io/recording_220923_123901.wav"></audio>
    </div>
    <div class="section">
        <h2>ML Workstation performance</h2>
        <p>The new "hippo" workstations with dual NVIDIA RTX-4090 GPUS have greatly improved performance over the elephants (which have 4 1080's or 2080s).</p>
        <p>For the same training task, they perform around eight times faster.   More importantly, the 24Gb memory on the GPUs makes it possible to train large networks that require more than 12 Gb GPU memory.</p>
        <p>Below is a comparison of training runs.  For both machines, the largest possible batch size was chosen.  The last column, Iters/sec, shows the relative speed of the two machines.</p>
        <table>
            <thead>
            <tr>
                <th>Machine</th>
                <th class="num">Steps</th>
                <th class="num">Batch Size</th>
                <th class="num">Iters</th>
                <th class="num">Time (days)</th>
                <th class="num">Iters/sec</th>
            </tr>
            </thead>
            <tbody>
            <tr>
                <td>elephantblue</td>
                <td class="num">449,000</td>
                <td class="num">8</td>
                <td class="num">3,592,000</td>
                <td class="num">3.02</td>
                <td class="num">13.76</td>
            </tr>
            <tr>
                <td >hippo1</td>
                <td class="num">269,000</td>
                <td class="num">64</td>
                <td class="num">17,216,000</td>
                <td class="num">1.78</td>
                <td class="num">111.89</td>

            </tr>
            </tbody>
        </table>
    </div>
    <div class="section">
        <h2>FreeVC Training</h2>
        <p>Our best-performing synthesis pipeline is based on FreeVC,  which has a similar architecture to our HuBERT/HifiGAN autoencoder, but with an additional speaker-embedding network, which improves noise robustness and maintains prosody/speech style.</p>
        <p>With the new hippos, it's now practical for us to train the FreeVC network on the denoising task (previously we've been using pretrained models).</p>
        <p>The three training runs we're done so far are:
        <ul>
            <li>Reproduce the training/results from the <a href="https://arxiv.org/abs/2210.15418">original FreeVC paper</a></li>
            <li>Train using a HuBERT encoder, replacing the WavLM encoder</li>
            <li>Train with the VCTK speech corpus with additional babble mix (data augmentation)</li>
        </ul>
        </p>
        <table>
            <thead>
            <tr>
                <th>Original</th>
                <th>Denoise -> autoencoder (HuBERT/HiFiGAN)</th>
                <th>FreeVC/WavLM</th>
                <th>FreeVC/HuBERT</th>
                <th>FreeVC/WavLM trained with babble data augmentation</th>

            </tr>
            </thead>
            <tbody>
            <tr>
                <td><audio controls src="media/cambridge_pub_recordings/pub_recordings/sample2_050723_183053/noisy_first_10_seconds.wav"></audio></td>
                <td><audio controls src="media/cambridge_pub_recordings/pub_recordings/sample2_050723_183053/hubert_large_emb_denoised.wav"></audio></td>
                <td><audio controls src="media/freevc/test_wavlm.wav"></audio></td>
                <td><audio controls src="media/freevc/test_hubert.wav"></audio></td>
                <td><audio controls src="media/freevc/test_wavlm_babble_mix.wav"></audio></td>

            </tr>
            </tbody>
        </table>

    </div>
    <hr/>

    <div>
        Recent catch-ups:
        <a href="index_20230908.html">2023-09-08</a>
        <a href="index_20230825.html">2023-08-25</a>
        <a href="index_20230818.html">2023-08-18</a>
    </div>
    <div>
        <a href="#older">Older catch-ups:</a>
        <div id="older">
            <a href="index_20230811.html">2023-08-11</a>
            <a href="index_20230804.html">2023-08-04</a>
            <a href="index_20230728.html">2023-07-28</a>
            <a href="index_20230721.html">2023-07-21</a>
            <a href="index_20230714.html">2023-07-14</a>
            <a href="index_20230707.html">2023-07-07</a>
            <a href="index_20230630.html">2023-06-30</a>
            <a href="index_20230623.html">2023-06-23</a>
            <a href="index_20230616.html">2023-06-16</a>
            <a href="index_20230609.html">2023-06-09</a>
            <a href="index_20230602.html">2023-06-02</a>
            <a href="index_20230526.html">2023-05-26</a>
            <a href="#">(Hide)</a>
        </div>
    </div>
</div>
</body>
</html>
